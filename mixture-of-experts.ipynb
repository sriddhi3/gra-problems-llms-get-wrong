{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "from utils import load_all_llm_answers_from_json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO\n",
    "- HEATMAP of each model and whether they get the same questions wrong or not\n",
    "- Add probabilistic outputs to each model answer\n",
    "- Add \"abstain\" option for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_save_path = './2024-07-20-Multi-Benchmark/auto_eval_outputs'\n",
    "\n",
    "all_llm_answers = load_all_llm_answers_from_json(answers_save_path, prefix_replace='auto_eval-')\n",
    "\n",
    "all_llm_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_llm_answers[list(all_llm_answers.keys())[0]].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sanity check to see if the scores are being calculated correctly in the below cells after a low of manipulation\n",
    "# model_scores = []\n",
    "# for llm_model in list(all_llm_answers.keys()):\n",
    "#     correct_answer = all_llm_answers[llm_model]['correct_letter'] == all_llm_answers[llm_model]['json_answer_letter']\n",
    "#     percentage_correct = correct_answer.value_counts(normalize=True)[True] * 100\n",
    "#     initial_score = all_llm_answers[llm_model]['score'].mean()\n",
    "#     dict_results = {\n",
    "#         'model': llm_model,\n",
    "#         'percentage_correct': percentage_correct,\n",
    "#         'initial_score': initial_score\n",
    "#     }\n",
    "#     model_scores.append(dict_results)\n",
    "\n",
    "# model_scores_series = pd.DataFrame(model_scores).sort_values(by='percentage_correct', ascending=False)\n",
    "# model_scores_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moe_llm_answers = {}\n",
    "for llm_model in list(all_llm_answers.keys()):\n",
    "    answer_subset = all_llm_answers[llm_model]\n",
    "    data = answer_subset[[\n",
    "        'index', 'question', 'multiple_choice', 'correct_answer',\n",
    "        'multi_choice_question', 'correct_letter', 'json_answer_letter',\n",
    "    ]]\n",
    "    moe_llm_answers[llm_model] = []\n",
    "    for idx, row in data.iterrows():\n",
    "        # if idx > 0:\n",
    "        #     continue\n",
    "        normalized_choices_dict_inv = dict(zip(sorted(row['multiple_choice']), ['A', 'B', 'C', 'D']))\n",
    "        presented_choices_dict = dict(zip(['A', 'B', 'C', 'D'], row['multiple_choice']))\n",
    "        correct_answer = presented_choices_dict[row['correct_letter']]\n",
    "        row['correct'] = row['correct_letter'] == row['json_answer_letter']\n",
    "        assert(row['correct_answer'] == correct_answer)\n",
    "        normalized_correct_letter = normalized_choices_dict_inv[correct_answer]\n",
    "        if row['json_answer_letter'] not in presented_choices_dict:\n",
    "            selected_answer, normalized_choice_letter = None, None\n",
    "        else:\n",
    "            selected_answer = presented_choices_dict[row['json_answer_letter']]\n",
    "            normalized_choice_letter = normalized_choices_dict_inv[selected_answer]\n",
    "        moe_llm_answers[llm_model].append({\n",
    "            'question': row['question'],\n",
    "            # 'selected_answer': selected_answer,\n",
    "            # 'correct_answer': correct_answer,\n",
    "            'correct': row['correct'],\n",
    "            'normalized_correct_letter': normalized_correct_letter,\n",
    "            'normalized_choice_letter': normalized_choice_letter,\n",
    "        })\n",
    "        #print(f\"idx: {idx} | {llm_model} | {moe_llm_answers[llm_model]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moe_llm_answers_comb_df = {}\n",
    "for llm_model in list(all_llm_answers.keys()):\n",
    "    data = pd.DataFrame(moe_llm_answers[llm_model])\n",
    "    data.reset_index(drop=False, inplace=True)\n",
    "    columns_mapping = {\n",
    "        'index': ('bench', 'index'),\n",
    "        'question': ('bench', 'question'),\n",
    "        'normalized_correct_letter': ('bench', 'normalized_correct_letter'),\n",
    "        'normalized_choice_letter': (llm_model, 'normalized_choice_letter'),\n",
    "        'correct': (llm_model, 'correct'),\n",
    "    }\n",
    "    grouped_columns_map = [columns_mapping[col] for col in data.columns if col in columns_mapping]\n",
    "    grouped_columns = pd.MultiIndex.from_tuples(grouped_columns_map)\n",
    "    data.columns = grouped_columns\n",
    "    data\n",
    "    if len(moe_llm_answers_comb_df) == 0:\n",
    "        moe_llm_answers_comb_df = data\n",
    "    else:\n",
    "        moe_llm_answers_comb_df = pd.merge(moe_llm_answers_comb_df, data, on=[\n",
    "            ('bench', 'index'), ('bench', 'question'), ('bench', 'normalized_correct_letter')\n",
    "        ])\n",
    "\n",
    "assert len(moe_llm_answers_comb_df) == len(all_llm_answers[list(all_llm_answers.keys())[0]]), 'Unable to reliably merge based on index, question, and normalized_correct_letter. This might suggest that the data is inconsistent or inaccurate'\n",
    "\n",
    "moe_llm_answers_comb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_models = list(all_llm_answers.keys())\n",
    "norm_choice_cols = list(zip(llm_models, ['normalized_choice_letter'] * len(llm_models)))\n",
    "norm_correct_cols = list(zip(llm_models, ['correct'] * len(llm_models)))\n",
    "norm_choice_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_correlations = {}\n",
    "for (model_base, col) in norm_choice_cols:\n",
    "    model_correlations = {}\n",
    "    for (model_test, col) in norm_choice_cols:\n",
    "        same_answer_percentage = ((moe_llm_answers_comb_df[model_base][col].fillna(0) \n",
    "                                   == moe_llm_answers_comb_df[model_test][col].fillna(0))\n",
    "                                  .value_counts(normalize=True)[True])\n",
    "        model_correlations[model_test] = same_answer_percentage\n",
    "    answer_correlations[model_base] = model_correlations\n",
    "\n",
    "correlation_matrix = pd.DataFrame(answer_correlations)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "ax = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=0, vmax=1, \n",
    "                 cbar_kws={\"shrink\": 0.5}, fmt='.2f', annot_kws={'size': 7})\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run test to see how MOE performs with various permutations of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to see if model scores match those calculated earlier (because we have done a lot of messing about). \n",
    "# It can used as a basis to see if MOE is improving results or not.\n",
    "result_series = pd.Series({col[0]: (moe_llm_answers_comb_df[col] == moe_llm_answers_comb_df['bench']['normalized_correct_letter']).value_counts(normalize=True)[True] \n",
    " for col in norm_choice_cols})\n",
    "\n",
    "result_series.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_most_common_or_best_model(row, best_model_rank=None):\n",
    "    # Get the value counts for the row, if all NaNs, return a random choice\n",
    "    value_counts = row.value_counts()\n",
    "    if value_counts.empty:  # Check if value_counts is empty (all values are NaN)\n",
    "        return np.random.choice(['A', 'B', 'C', 'D'])\n",
    "    else:\n",
    "        # Find the maximum count\n",
    "        max_count = value_counts.max()\n",
    "        # Find the indices (letters) that have the maximum count\n",
    "        most_common_choices = value_counts[value_counts == max_count].index.tolist()\n",
    "        # If there's more than one most frequent letter, pick one given rules\n",
    "        if len(most_common_choices) > 1:\n",
    "            if best_model_rank is not None:\n",
    "                for best_model in best_model_rank:\n",
    "                    if row[best_model]['normalized_choice_letter'] in most_common_choices:\n",
    "                        return row[best_model]['normalized_choice_letter']\n",
    "            else:\n",
    "                return np.random.choice(most_common_choices)\n",
    "        else:\n",
    "            return most_common_choices[0]\n",
    "\n",
    "# # Remove '['normalized_choice_letter']' from return function to run mock test below        \n",
    "# model_data_test = pd.DataFrame([\n",
    "#     {'One': 'A', 'Two': 'A', 'Three': 'B', 'Four': 'B'},\n",
    "#     {'One': 'C', 'Two': 'B', 'Three': 'C', 'Four': 'B'},\n",
    "#     {'One': 'A', 'Two': 'B', 'Three': 'C', 'Four': 'D'},\n",
    "#     ])\n",
    "\n",
    "# best_model_rank = ['Four', 'Three', 'Two', 'One']\n",
    "# most_common_answer = model_data_test.apply(axis='columns', \n",
    "#     func=lambda x: get_most_common_or_best_model(x, best_model_rank), result_type='expand')\n",
    "\n",
    "# most_common_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"QUESTION\\nA group of four people needs to cross a bridge at night. The bridge is very old and rickety. They have only one torch and because it's night-time, the torch is necessary to cross the bridge. Each person walks at a different speed: - A takes 1 minute to cross, - B takes 2 minutes, - C takes 5 minutes, and - D takes 10 minutes. What is the fastest time they can all get across the bridge?\\n\\nANSWERS\\nA. 17 minutes\\nB. 14 minutes\\nC. There is not enough information to determine the fastest time.\\nD. 10 minutes\\n\\nProvide an explanation for your thinking and then select a single choice from ANSWERS that answer the QUESTION. Return in JSON format, for example:\\n{\\\"ANSWER\\\": \\\"A\\\"}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_of_models = [\n",
    "                    # 'Meta-Llama-3-1-405B-Instruct-jjo_eastus_models_ai_azure_com',\n",
    "                    # 'claude-3-5-sonnet-20240620', \n",
    "                    'claude-3-opus-20240229',\n",
    "                    'o1-preview',\n",
    "                    # 'gemini-1_5-pro', \n",
    "                    # 'gpt-4o', \n",
    "                    # 'gpt-4-turbo-preview',\n",
    "                   ]\n",
    "subset_norm_choice_cols = [col for col in norm_choice_cols if col[0] in subset_of_models]\n",
    "subset_norm_correct_cols = [col for col in norm_correct_cols if col[0] in subset_of_models]\n",
    "\n",
    "# subset_norm_choice_cols = norm_choice_cols\n",
    "# subset_norm_correct_cols = norm_correct_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_correct_df = (\n",
    "    moe_llm_answers_comb_df.reset_index()[subset_norm_correct_cols + [('bench', 'question'), ('bench', 'index')]]\n",
    "    .groupby([('bench', 'question')])\n",
    "    .agg({\n",
    "        **{(col, 'correct') : 'mean' for col in subset_of_models},\n",
    "        ('bench', 'index'): 'min',\n",
    "    })\n",
    "    .sort_values(('bench', 'index'))\n",
    "    .set_index(('bench', 'index'))\n",
    ")\n",
    "question_correct_df.index += 1\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "cmap = sns.color_palette(\"Greys_r\", as_cmap=True)\n",
    "sns.heatmap(question_correct_df.T, cmap=cmap, cbar=False, linewidths=1, linecolor='grey')\n",
    "#plt.xticks(rotation=0, ha='right')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_correct_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model_rank = None #list(result_series.sort_values(ascending=False).index.values)\n",
    "\n",
    "answer_values = moe_llm_answers_comb_df.apply(axis='columns', \n",
    "    func=lambda x: x[subset_norm_choice_cols].value_counts())\n",
    "most_common_answer = moe_llm_answers_comb_df.apply(axis='columns', \n",
    "    func=lambda x: get_most_common_or_best_model(x[subset_norm_choice_cols], best_model_rank), result_type='expand')\n",
    "largest_common_answer = moe_llm_answers_comb_df.apply(axis='columns', \n",
    "    func=lambda x: x[subset_norm_choice_cols].value_counts().max())\n",
    "answer_values['correct_letter'] = moe_llm_answers_comb_df['bench']['normalized_correct_letter']\n",
    "answer_values['most_common_answer'] = most_common_answer\n",
    "answer_values['largest_common_answer'] = largest_common_answer\n",
    "display(answer_values)\n",
    "\n",
    "correct = (answer_values['correct_letter'] == answer_values['most_common_answer']).sum() / len(answer_values)\n",
    "print('Score %:', correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_values['correct'] = (answer_values['correct_letter'] == answer_values['most_common_answer']).astype(int)\n",
    "accuracy_by_vote_agg = answer_values[['largest_common_answer', 'correct']].groupby('largest_common_answer')\n",
    "accuracy_by_vote_agg.mean().plot(kind='bar', title='Accuracy by Most Common Answer')\n",
    "accuracy_by_vote_agg.count().plot(kind='bar', title='Accuracy by Most Common Answer')\n",
    "print(f'Highest accuracy: {accuracy_by_vote_agg.mean().max().values[0]:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def all_combinations(arr):\n",
    "    x = len(arr)\n",
    "    all_combos = []\n",
    "    for r in range(2, x + 1):\n",
    "        combos = list(combinations(arr, r))\n",
    "        all_combos.extend(combos)\n",
    "    return all_combos\n",
    "\n",
    "all_combos = all_combinations(llm_models)\n",
    "all_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_combo_results = {}\n",
    "for models in tqdm(all_combos):\n",
    "    subset_norm_choice_cols = [col for col in norm_choice_cols if col[0] in models]\n",
    "    answer_values = moe_llm_answers_comb_df.apply(axis='columns', \n",
    "        func=lambda x: x[subset_norm_choice_cols].value_counts())\n",
    "    most_common_answer = moe_llm_answers_comb_df.apply(axis='columns', \n",
    "        func=lambda x: get_most_common_or_best_model(x[subset_norm_choice_cols]), result_type='expand')\n",
    "    largest_common_answer = moe_llm_answers_comb_df.apply(axis='columns', \n",
    "        func=lambda x: x[subset_norm_choice_cols].value_counts().max())\n",
    "    answer_values['correct_letter'] = moe_llm_answers_comb_df['bench']['normalized_correct_letter']\n",
    "    answer_values['most_common_answer'] = most_common_answer\n",
    "    answer_values['largest_common_answer'] = largest_common_answer\n",
    "    #display(answer_values)\n",
    "\n",
    "    correct = (answer_values['correct_letter'] == answer_values['most_common_answer']).sum() / len(answer_values)\n",
    "    #print(f'Models: {models} | Score %: {correct}')\n",
    "    all_combo_results[models] = correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "for (models, score) in all_combo_results.items():\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        print(f'Models: {models} | Score %: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_combo_results, index=['score']).T.sort_values(by='score', ascending=False)#.to_csv('./all_combo_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
